{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PROCESSING\n",
        "Note: Cell below will clean the data and merge all the datasets together into an excel file called \"merged_data.xlsx\"."
      ],
      "metadata": {
        "id": "MVSXtR6KpTvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PROCESSING\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load datasets\n",
        "# NOTE: Modify the paths before running.\n",
        "deaths_air_pollution = pd.read_excel(\"/content/air pollution deaths.xlsx\")\n",
        "hepb3_immunized = pd.read_excel(\"/content/immunization of hepb3 of one year old children.xlsx\")\n",
        "deaths_infectious_diseases = pd.read_excel(\"/content/infectious diseases deaths.xlsx\")\n",
        "life_expectancy = pd.read_excel(\"/content/life expectancy.xlsx\")\n",
        "deaths_smoking = pd.read_excel(\"/content/number of deaths from tobacco smoking.xlsx\")\n",
        "under_fifteen_mortality = pd.read_excel(\"/content/number of youth deaths.xlsx\")\n",
        "deaths_ozone_pollution = pd.read_excel(\"/content/ozone pollution deaths.xlsx\")\n",
        "health_expenditure = pd.read_excel(\"/content/total healthcare expenditure as share of gdp.xlsx\")\n",
        "\n",
        "# Rename variables\n",
        "deaths_air_pollution.rename(columns={\"Deaths that are from all causes attributed to air pollution, in both sexes aged all ages\": \"deaths_air_poll\"}, inplace=True)\n",
        "hepb3_immunized.rename(columns={\"HepB3 (% of one-year-olds immunized)\": \"hepb3_imm\"}, inplace=True)\n",
        "deaths_infectious_diseases.rename(columns={\"Deaths from Infectious diseases\": \"deaths_infect_dis\"}, inplace=True)\n",
        "life_expectancy.rename(columns={\"Life expectancy at birth\": \"life_exp\"}, inplace=True)\n",
        "deaths_smoking.rename(columns={\"Deaths that are from all causes attributed to smoking\": \"deaths_smoking\"}, inplace=True)\n",
        "under_fifteen_mortality.rename(columns={\"Under fifteen mortality - Number of deaths \": \"under15_mort\"}, inplace=True)\n",
        "deaths_ozone_pollution.rename(columns={\"Deaths that are from all causes attributed to ambient ozone pollution\": \"deaths_ozone_poll\"}, inplace=True)\n",
        "health_expenditure.rename(columns={\"Current health expenditure (CHE) as percentage of gross domestic product (GDP) (%)\": \"health_exp\"}, inplace=True)\n",
        "\n",
        "# Drop 'Code' column from each DataFrame\n",
        "deaths_air_pollution.drop(columns=['Code'], inplace=True)\n",
        "hepb3_immunized.drop(columns=['Code'], inplace=True)\n",
        "deaths_infectious_diseases.drop(columns=['Code'], inplace=True)\n",
        "life_expectancy.drop(columns=['Code'], inplace=True)\n",
        "deaths_smoking.drop(columns=['Code'], inplace=True)\n",
        "under_fifteen_mortality.drop(columns=['Code'], inplace=True)\n",
        "deaths_ozone_pollution.drop(columns=['Code'], inplace=True)\n",
        "health_expenditure.drop(columns=['Code'], inplace=True)\n",
        "\n",
        "common_years = range(1990, 2015)\n",
        "health_expenditure = health_expenditure[(health_expenditure['Year'] >= 2002)]\n",
        "\n",
        "# Merge datasets\n",
        "merged_data = deaths_air_pollution.merge(hepb3_immunized, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(deaths_infectious_diseases, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(life_expectancy, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(deaths_smoking, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(under_fifteen_mortality, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(deaths_ozone_pollution, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(health_expenditure, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "\n",
        "merged_data.reset_index(drop=True, inplace=True)\n",
        "merged_data.to_excel(\"merged_data.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "5qRgj0uHpcta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING\n",
        "Note: Make sure to get \"merged_data.xlsx\" before running the cell below. You can get it by running the data processing cell above."
      ],
      "metadata": {
        "id": "0VhHaXo9nw5B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLHNwoC-ghs3",
        "outputId": "86a8695e-53e1-4ab8-c9dd-9f46b2b51cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 2ms/step\n",
            "Mean Squared Error: 3.469713391815065\n",
            "Mean Absolute Error: 1.2331140922321993\n",
            "R-squared: 0.5612553632893627\n",
            "\n",
            "Top 10 countries with highest predicted expenditure:\n",
            "1. United States\n",
            "2. Nauru\n",
            "3. Marshall Islands\n",
            "4. Niue\n",
            "5. Tuvalu\n",
            "6. Sierra Leone\n",
            "7. Fiji\n",
            "8. France\n",
            "9. Netherlands\n",
            "10. Germany\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import pickle\n",
        "\n",
        "# Load the dataset\n",
        "# NOTE: Modify the path before running.\n",
        "dataset_path = \"/content/merged_data.xlsx\"\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Simplify features: For simplicity, let's focus on using only the most recent year's data for each feature\n",
        "df_simplified = df[['deaths_air_poll', 'hepb3_imm',\n",
        "       'deaths_infect_dis', 'life_exp', 'deaths_smoking', 'under15_mort',\n",
        "       'deaths_ozone_poll', 'health_exp']]\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_simplified.drop(columns=['health_exp'])\n",
        "y = df_simplified['health_exp']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Load the trained model\n",
        "with open('Patel_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"R-squared:\", r2)\n",
        "print()\n",
        "\n",
        "# Combine predictions with countries\n",
        "predictions_df = pd.DataFrame({'Entity': df.loc[y_val.index, 'Entity'], 'health_exp_predicted': y_pred.flatten()})\n",
        "\n",
        "# Identify top 10 countries with highest predicted expenditure\n",
        "top_10_countries = predictions_df.groupby('Entity')['health_exp_predicted'].mean().nlargest(10).index.tolist()\n",
        "print(\"Top 10 countries with highest predicted expenditure:\")\n",
        "for i, country in enumerate(top_10_countries, 1):\n",
        "    print(f\"{i}. {country}\")"
      ]
    }
  ]
}