{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PROCESSING\n",
        "Note: Cell below will clean the data and merge all the datasets together into an excel file called \"merged_data.xlsx\"."
      ],
      "metadata": {
        "id": "zWcflGaMmzfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PROCESSING\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load datasets\n",
        "# NOTE: Modify the paths before running.\n",
        "deaths_air_pollution = pd.read_excel(\"/content/air pollution deaths.xlsx\")\n",
        "hepb3_immunized = pd.read_excel(\"/content/immunization of hepb3 of one year old children.xlsx\")\n",
        "deaths_infectious_diseases = pd.read_excel(\"/content/infectious diseases deaths.xlsx\")\n",
        "life_expectancy = pd.read_excel(\"/content/life expectancy.xlsx\")\n",
        "deaths_smoking = pd.read_excel(\"/content/number of deaths from tobacco smoking.xlsx\")\n",
        "under_fifteen_mortality = pd.read_excel(\"/content/number of youth deaths.xlsx\")\n",
        "deaths_ozone_pollution = pd.read_excel(\"/content/ozone pollution deaths.xlsx\")\n",
        "health_expenditure = pd.read_excel(\"/content/total healthcare expenditure as share of gdp.xlsx\")\n",
        "\n",
        "# Rename variables\n",
        "deaths_air_pollution.rename(columns={\"Deaths that are from all causes attributed to air pollution, in both sexes aged all ages\": \"deaths_air_poll\"}, inplace=True)\n",
        "hepb3_immunized.rename(columns={\"HepB3 (% of one-year-olds immunized)\": \"hepb3_imm\"}, inplace=True)\n",
        "deaths_infectious_diseases.rename(columns={\"Deaths from Infectious diseases\": \"deaths_infect_dis\"}, inplace=True)\n",
        "life_expectancy.rename(columns={\"Life expectancy at birth\": \"life_exp\"}, inplace=True)\n",
        "deaths_smoking.rename(columns={\"Deaths that are from all causes attributed to smoking\": \"deaths_smoking\"}, inplace=True)\n",
        "under_fifteen_mortality.rename(columns={\"Under fifteen mortality - Number of deaths \": \"under15_mort\"}, inplace=True)\n",
        "deaths_ozone_pollution.rename(columns={\"Deaths that are from all causes attributed to ambient ozone pollution\": \"deaths_ozone_poll\"}, inplace=True)\n",
        "health_expenditure.rename(columns={\"Current health expenditure (CHE) as percentage of gross domestic product (GDP) (%)\": \"health_exp\"}, inplace=True)\n",
        "\n",
        "# Drop 'Code' column from each DataFrame\n",
        "deaths_air_pollution.drop(columns=['Code'], inplace=True)\n",
        "hepb3_immunized.drop(columns=['Code'], inplace=True)\n",
        "deaths_infectious_diseases.drop(columns=['Code'], inplace=True)\n",
        "life_expectancy.drop(columns=['Code'], inplace=True)\n",
        "deaths_smoking.drop(columns=['Code'], inplace=True)\n",
        "under_fifteen_mortality.drop(columns=['Code'], inplace=True)\n",
        "deaths_ozone_pollution.drop(columns=['Code'], inplace=True)\n",
        "health_expenditure.drop(columns=['Code'], inplace=True)\n",
        "\n",
        "common_years = range(1990, 2015)\n",
        "health_expenditure = health_expenditure[(health_expenditure['Year'] >= 2002)]\n",
        "\n",
        "# Merge datasets\n",
        "merged_data = deaths_air_pollution.merge(hepb3_immunized, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(deaths_infectious_diseases, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(life_expectancy, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(deaths_smoking, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(under_fifteen_mortality, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(deaths_ozone_pollution, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "merged_data = merged_data.merge(health_expenditure, on=[\"Entity\", \"Year\"], how=\"inner\")\n",
        "\n",
        "merged_data.reset_index(drop=True, inplace=True)\n",
        "merged_data.to_excel(\"merged_data.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "MQOwqbfogpLT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING\n",
        "\n",
        "Note: Make sure you have \"merged_data.xlsx\" (You can get it by running the Data Processing cell) before running the cell below."
      ],
      "metadata": {
        "id": "Z8NenmvRm345"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2HPr1FVGgcJD"
      },
      "outputs": [],
      "source": [
        "# TRAINING\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = \"/content/merged_data.xlsx\"\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Simplify features\n",
        "df_simplified = df[['deaths_air_poll', 'hepb3_imm',\n",
        "       'deaths_infect_dis', 'life_exp', 'deaths_smoking', 'under15_mort',\n",
        "       'deaths_ozone_poll', 'health_exp']]\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_simplified.drop(columns=['health_exp'])\n",
        "y = df_simplified['health_exp']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=500, batch_size=64, validation_data=(X_val_scaled, y_val), verbose=0)\n",
        "\n",
        "# Save the trained model\n",
        "with open('Patel_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    }
  ]
}